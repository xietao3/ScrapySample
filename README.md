![å›¾ç‰‡æ¥è‡ªç½‘ç»œ](https://user-gold-cdn.xitu.io/2018/1/26/1613309f74eee5df?imageView2/1/w/1304/h/734/q/85/interlace/1)

>Python æ–°æ‰‹å…¥é—¨å¾ˆå¤šæ—¶å€™éƒ½ä¼šå†™ä¸ªçˆ¬è™«ç»ƒæ‰‹ï¼Œæœ¬æ•™ç¨‹ä½¿ç”¨ Scrapy æ¡†æ¶ï¼Œå¸®ä½ ç®€å•å¿«é€Ÿå®ç°çˆ¬è™«ï¼Œå¹¶å°†æ•°æ®ä¿å­˜è‡³æ•°æ®åº“ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­æ•°æ®æŒ–æ˜ä¹Ÿæ˜¯ååˆ†é‡è¦çš„ï¼Œæˆ‘çš„æ•°æ®ç§‘å­¦è€å¸ˆæ›¾ç»è¯´è¿‡ï¼Œå¥½ç®—æ³•ä¸å¦‚å¥½æ•°æ®ã€‚

# ä»‹ç»

[Scrapy](https://scrapy.org/) ï¼ŒPython å¼€å‘çš„ä¸€ä¸ªå¿«é€Ÿã€é«˜å±‚æ¬¡çš„å±å¹•æŠ“å–å’Œ web æŠ“å–æ¡†æ¶ï¼Œç”¨äºæŠ“å– web ç«™ç‚¹å¹¶ä»é¡µé¢ä¸­æå–ç»“æ„åŒ–çš„æ•°æ®ã€‚æ–‡ä»¶ç»“æ„æ¸…æ™°ï¼Œå³ä½¿æ˜¯å°ç™½ä¹Ÿèƒ½å¤Ÿå¿«é€Ÿä¸Šæ‰‹ï¼Œæ€»ä¹‹éå¸¸å¥½ç”¨ğŸ˜‚ã€‚

[XPath](https://baike.baidu.com/item/XPath/5574064?fr=aladdin) ,å®ƒæ˜¯ä¸€ç§ç”¨æ¥æŸ¥æ‰¾ XML æ–‡æ¡£ä¸­èŠ‚ç‚¹ä½ç½®çš„è¯­è¨€ã€‚ XPath åŸºäº XML çš„æ ‘çŠ¶ç»“æ„ï¼Œæœ‰ä¸åŒç±»å‹çš„èŠ‚ç‚¹ï¼ŒåŒ…æ‹¬å…ƒç´ èŠ‚ç‚¹ï¼Œå±æ€§èŠ‚ç‚¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼Œæä¾›åœ¨æ•°æ®ç»“æ„æ ‘ä¸­æ‰¾å¯»èŠ‚ç‚¹çš„èƒ½åŠ›ã€‚

[MySQL](https://www.mysql.com/) æ˜¯ä¸€ç§å…³ç³»æ•°æ®åº“ç®¡ç†ç³»ç»Ÿï¼Œå®ƒçš„ä¼˜åŠ¿ï¼šå®ƒæ˜¯å…è´¹çš„ã€‚ä½œè€…æ˜¯ä¸‹è½½äº† [MAMP for Mac](https://www.mamp.info/en/) ï¼Œå†…åµŒ``MySQL``å’Œ``Apache``ã€‚

é¦–å…ˆé€šè¿‡ Scrapy çˆ¬å–åˆ°ç½‘é¡µåï¼Œ é€šè¿‡ XPath æ¥å®šä½æŒ‡å®šå…ƒç´ ï¼Œè·å–åˆ°ä½ æƒ³è¦çš„æ•°æ®,å¾—åˆ°æ•°æ®ä¹‹åå¯ä»¥å°†æ•°æ®å­˜å…¥æ•°æ®åº“( MySQL )ã€‚ç®€å•äº†è§£ä¹‹åå°±å¯ä»¥å¼€å§‹ç¼–å†™ä½ çš„çˆ¬è™«ã€‚

***é‡è¦**ï¼šä¸‹è½½å¹¶æŸ¥çœ‹ [Demo](https://github.com/xietao3/ScrapySample) ï¼Œç»“åˆæœ¬æ–‡å¯ä»¥å¿«é€Ÿå®ç°ä¸€ä¸ªåŸºæœ¬çˆ¬è™«âœŒï¸ã€‚

# å‡†å¤‡å·¥ä½œ

å®‰è£… Scrapy ``(ç³»ç»Ÿé»˜è®¤å®‰è£…äº† Python)``:
```
$ pip install Scrapy
```
åœ¨å½“å‰ç›®å½•æ–°å»ºå·¥ç¨‹
```
$ scrapy startproject yourproject
```
æ–°å»ºå·¥ç¨‹æ–‡ä»¶ç»“æ„å¦‚ä¸‹ï¼š
```
yourproject/
----|scrapy.cfg             # éƒ¨ç½²é…ç½®æ–‡ä»¶
    |yourproject/           # å·¥ç¨‹ç›®å½•
    |----__init__.py
    |----items.py           # é¡¹ç›®æ•°æ®æ–‡ä»¶
    |----pipelines.py       # é¡¹ç›®ç®¡é“æ–‡ä»¶
    |----settings.py        # é¡¹ç›®è®¾ç½®æ–‡ä»¶
    |----spiders/           # æˆ‘ä»¬çš„çˆ¬è™« ç›®å½•
        |----__init__.py    # çˆ¬è™«ä¸»è¦ä»£ç åœ¨è¿™é‡Œ    
```


ç®€å•çš„çˆ¬è™«ä¸»è¦ä½¿ç”¨äº†``spiders``ã€``items``ã€``pipelines``è¿™ä¸‰ä¸ªæ–‡ä»¶ï¼š

* spider ï¼šçˆ¬è™«çš„ä¸»è¦é€»è¾‘ã€‚
* items ï¼šçˆ¬è™«çš„æ•°æ®æ¨¡å‹ã€‚
* pipelines ï¼š çˆ¬è™«è·å–åˆ°çš„æ•°æ®çš„åŠ å·¥å·¥å‚ï¼Œå¯ä»¥è¿›è¡Œæ•°æ®ç­›é€‰æˆ–ä¿å­˜ã€‚

# æ•°æ®æ¨¡å‹ï¼šitems

![items](https://user-gold-cdn.xitu.io/2018/1/26/16132e37082a8831?w=300&h=300&f=jpeg&s=12764)

å…ˆçœ‹çœ‹æˆ‘ä»¬è¦çˆ¬å–çš„[ç½‘ç«™](http://quotes.toscrape.com)ï¼Œè¿™ä¸ªæ˜¯ Scrapy å®˜æ–¹ Demo çˆ¬å–æ•°æ®ç”¨çš„ç½‘ç«™ï¼Œæˆ‘ä»¬å…ˆç”¨è¿™ä¸ªæ¥ç»ƒæ‰‹ã€‚

![quotes](https://user-gold-cdn.xitu.io/2018/1/24/16128781a79a30f4?w=1774&h=1134&f=jpeg&s=250425)

åˆ†æç½‘é¡µçš„ä¿¡æ¯æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç½‘é¡µä¸»ä½“æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨æ¯ä¸€è¡Œéƒ½åŒ…å«å¯ä¸€å¥å¼•ç”¨ã€ä½œè€…åå­—ã€æ ‡ç­¾ç­‰ä¿¡æ¯ã€‚ä½œè€…åå³è¾¹ç‚¹å‡»ï¼ˆaboutï¼‰å¯ä»¥çœ‹åˆ°ä½œè€…çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä»‹ç»ã€å‡ºç”Ÿå¹´æœˆæ—¥ã€åœ°ç‚¹ç­‰ç­‰ã€‚æ ¹æ®ä¸Šé¢çš„æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆåˆ›å»ºå¦‚ä¸‹æ•°æ®æ¨¡å‹ï¼š

**items.py**

```
import scrapy

# quote æˆ‘ä»¬è¦çˆ¬å–çš„ä¸»ä½“
class QuoteItem(scrapy.Item):
    text = scrapy.Field()
    tags = scrapy.Field()
    author = scrapy.Field()

    next_page = scrapy.Field()
    pass
    
# quote çš„ä½œè€…ä¿¡æ¯ å¯¹åº” QuoteItem.author
class AuthorItem(scrapy.Item):
    name = scrapy.Field()
    birthday = scrapy.Field()
    address = scrapy.Field()
    description = scrapy.Field()
    pass
```

æ‰€æœ‰çš„æ¨¡å‹å¿…é¡»ç»§æ‰¿``scrapy.Item``ï¼Œå®Œæˆè¿™ä¸€æ­¥æˆ‘ä»¬å°±å¯ä»¥å¼€å§‹å†™çˆ¬è™«çš„é€»è¾‘äº†ã€‚

```
# å®Œæ•´çš„ QuoteItem æ•°æ®ç»“æ„ç¤ºä¾‹
{
    text,
    tags,
    author:{
        name,
        birthday,
        address,
        description
    }
}
```

# çˆ¬è™«ï¼šspider

![Spider](https://user-gold-cdn.xitu.io/2018/1/26/16132e022f073f10?w=200&h=200&f=jpeg&s=17148)

æ—¢ç„¶æ˜¯çˆ¬è™«ï¼Œè‡ªç„¶éœ€è¦å»çˆ¬å–ç½‘é¡µï¼Œçˆ¬è™«éƒ¨åˆ†çš„å‡ ä¸ªè¦ç‚¹ï¼š

1. å¼•å…¥ä½ åˆ›å»ºçš„æ•°æ®æ¨¡å‹
2. é¦–å…ˆçˆ¬è™«ç±»è¦ç»§æ‰¿``scrapy.Spider``ã€‚
3. è®¾ç½®çˆ¬è™«çš„åå­—``name``ï¼Œå¯åŠ¨çˆ¬è™«æ—¶è¦ç”¨ã€‚
4. å°†ä½ è¦çˆ¬å–çš„ç½‘å€æ”¾å…¥``start_requests()``ï¼Œä½œä¸ºçˆ¬è™«çš„èµ·ç‚¹ã€‚
5. çˆ¬å–çš„ç½‘é¡µä¿¡æ¯æˆåŠŸåï¼Œåœ¨çš„è¯·æ±‚å“åº”``parse()``ä¸­è§£æã€‚


**spiders/__init__.py**

* **åœ¨é¡¶éƒ¨å¼•å…¥åˆ›å»ºçš„æ•°æ®æ¨¡å‹ã€‚**

```
import scrapy
from ScrapySample.items import QuoteItem
from ScrapySample.items import AuthorItem
```


* **çˆ¬è™«ç±»ï¼Œ``name``->çˆ¬è™«åå­—ï¼Œ``allowed_domains``->çˆ¬å–ç½‘é¡µçš„ç™½åå•ã€‚**

```
class QuotesSpider(scrapy.Spider):
    name = "quotes"
    allowed_domains = ["quotes.toscrape.com"]
```

* **åœ¨``start_requests()``ä¸­è®°å½•ä½ è¦çˆ¬å–çš„ç½‘å€ã€‚**

    å¯ä»¥åªæ”¾å…¥ä¸€ä¸ªç½‘å€ï¼Œç„¶åè®©çˆ¬è™«è‡ªå·±çˆ¬å–èµ·å§‹ç½‘å€ä¸­ä¸‹ä¸€é¡µçš„é“¾æ¥ã€‚ä¹Ÿå¯ä»¥åœ¨è¿™é‡ŒæŠŠæ‰€æœ‰éœ€è¦çˆ¬çš„ç½‘å€ç›´æ¥æ”¾å…¥ï¼Œæ¯”å¦‚è¯´``page``ä¸€èˆ¬æ˜¯ä»1å¼€å§‹ï¼Œå¹¶ä¸”æœ‰åºçš„ï¼Œå†™ä¸€ä¸ª``for``å¾ªç¯å¯ä»¥ç›´æ¥è¾“å…¥æ‰€æœ‰é¡µé¢çš„ç½‘å€ã€‚

    æœ¬æ–‡ä½¿ç”¨çš„æ˜¯è®©çˆ¬è™«è‡ªå·±å»çˆ¬å–ä¸‹ä¸€é¡µç½‘å€çš„æ–¹å¼ï¼Œæ‰€ä»¥åªå†™å…¥äº†ä¸€ä¸ªèµ·å§‹ç½‘å€ã€‚

```
    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)
```

* **å¦‚ä¸‹ä»£ç ï¼Œçˆ¬å–ç½‘é¡µæˆåŠŸä¹‹åï¼Œæˆ‘ä»¬è¦åˆ†æç½‘é¡µç»“æ„ï¼Œæ‰¾åˆ°æˆ‘ä»¬éœ€è¦çš„æ•°æ®ã€‚**

    æˆ‘ä»¬å…ˆæ¥çœ‹XPathè¯­æ³•ï¼Œ``//div[@class="col-md-8"]/div[@class="quote"``ï¼šè¿™æ˜¯è¡¨ç¤ºæŸ¥æ‰¾ class ä¸º``"col-md-8"``çš„ div èŠ‚ç‚¹ä¸‹çš„ä¸€ä¸ªå­èŠ‚ç‚¹ï¼Œå¹¶ä¸”å­èŠ‚ç‚¹æ˜¯ä¸€ä¸ª class ä¸º``"quote"`` div èŠ‚ç‚¹ã€‚å¦‚æœåœ¨å½“å‰é¡µé¢æ‰¾åˆ°äº†è¿™æ ·ä¸€ä¸ªèŠ‚ç‚¹ï¼Œåˆ™è¿”å›èŠ‚ç‚¹ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›``None``ã€‚

```
    def parse(self, response):
        # é€šè¿‡æŸ¥çœ‹å™¨ï¼Œæ‰¾åˆ°åˆ—è¡¨æ‰€åœ¨çš„èŠ‚ç‚¹
        courses = response.xpath('//div[@class="col-md-8"]/div[@class="quote"]')

        for course in courses:
            # å°†æ•°æ®æ¨¡å‹å®ä¾‹åŒ– å¹¶ä»èŠ‚ç‚¹ä¸­æ‰¾åˆ°æ•°æ®å¡«å…¥æˆ‘ä»¬çš„æ•°æ®æ¨¡å‹
            item = QuoteItem()
            # è½®è¯¢ course èŠ‚ç‚¹ä¸‹æ‰€æœ‰ class ä¸º "text" çš„ span èŠ‚ç‚¹,è·å–æ‰€æœ‰åŒ¹é…åˆ°çš„èŠ‚ç‚¹çš„ text() ï¼Œç”±äºè·å–åˆ°çš„æ˜¯åˆ—è¡¨ï¼Œæˆ‘ä»¬é»˜è®¤å–ç¬¬ä¸€ä¸ªã€‚
            item['text'] = course.xpath('.//span[@class="text"]/text()').extract_first()
            item['author'] = course.xpath('.//small[@class="author"]/text()').extract_first()
            item['tags'] = course.xpath('.//div[@class="tags"]/a/text()').extract()

            # è¯·æ±‚ä½œè€…è¯¦ç»†ä¿¡æ¯
            author_url = course.xpath('.//a/@href').extract_first()
            # å¦‚æœä½œè€…ä»‹ç»çš„é“¾æ¥ä¸ä¸ºç©º åˆ™å»è¯·æ±‚ä½œè€…çš„è¯¦ç»†ä¿¡æ¯
            if author_url != '':
                request = scrapy.Request(url='http://quotes.toscrape.com'+author_url, dont_filter=True, callback=self.authorParse)
                # å°†æˆ‘ä»¬å·²ç»è·å–åˆ°çš„ QuoteItem ä¼ å…¥è¯¥è¯·æ±‚çš„å›è°ƒå‡½æ•° authorParse()ï¼Œåœ¨è¯¥å‡½æ•°å†…ç»§ç»­å¤„ç†ä½œè€…ç›¸å…³æ•°æ®ã€‚
                request.meta['item'] = item
                yield request
        
        # ç»§ç»­çˆ¬å‘ä¸‹ä¸€é¡µ è¯¥å‡½æ•°å…·ä½“å®ç°ä¸‹é¢ä¼šåˆ†æ
        next_page_request = self.requestNextPage(response)
        yield next_page_request
```

*è¿™æ®µæ³¨é‡Šä¸æ˜¯å¾ˆè¯¦ç»†ï¼Œå¦‚æœçœ‹ä¸æ‡‚å¯èƒ½éœ€è¦è¡¥ä¸€ä¸‹ç›¸å…³çŸ¥è¯†ã€‚*


* **çˆ¬å–ä½œè€…è¯¦ç»†ä¿¡æ¯**

   æˆåŠŸè·å–ä½œè€…è¯¦ç»†ä¿¡æ¯ AuthorItem åå¹¶ä¸”èµ‹å€¼ç»™ QuoteItem çš„å±æ€§ ``author`` ï¼Œè¿™æ ·ä¸€ä¸ªå®Œæ•´çš„å¼•è¿°ä¿¡æ¯ QuoteItem å°±ç»„è£…å®Œæˆäº†ã€‚
    

```
    def authorParse(self, response):
        # å…ˆè·å–ä» parse() ä¼ é€’è¿‡æ¥çš„ QuoteItem
        item = response.meta['item']
        # é€šè¿‡æŸ¥çœ‹å™¨ï¼Œæ‰¾åˆ°ä½œè€…è¯¦ç»†ä¿¡æ¯æ‰€åœ¨èŠ‚ç‚¹
        sources = response.xpath('//div[@class="author-details"]')
        
        # å®ä¾‹åŒ–ä¸€ä¸ªä½œè€…ä¿¡æ¯çš„æ•°æ®æ¨¡å‹
        author_item = AuthorItem()
        # å¾€ä½œè€…ä¿¡æ¯æ¨¡å‹å¡«å…¥æ•°æ®
        for source in sources:
            author_item['name'] = source.xpath('.//h3[@class="author-title"]/text()').extract_first()
            author_item['birthday'] = source.xpath('.//span[@class="author-born-date"]/text()').extract_first()
            author_item['address'] = source.xpath('.//span[@class="author-born-location"]/text()').extract_first()
            author_item['description'] = source.xpath('.//div[@class="author-description"]/text()').extract_first()
    
        # æœ€åå°†ä½œè€…ä¿¡æ¯ author_item å¡«å…¥ QuoteItem 
        item['author'] = author_item
        # ä¿å­˜ç»„è£…å¥½çš„å®Œæ•´æ•°æ®æ¨¡å‹
        yield item
```

* **çˆ¬è™«è‡ªå·±æ‰¾åˆ°å‡ºè·¯ï¼ˆä¸‹ä¸€é¡µç½‘é¡µé“¾æ¥ï¼‰**

    é€šè¿‡æŸ¥çœ‹å™¨æˆ‘ä»¬å¯ä»¥æ‰¾åˆ°``ä¸‹ä¸€é¡µ``æŒ‰é’®å…ƒç´ ï¼Œæ‰¾åˆ°è¯¥èŠ‚ç‚¹å¹¶æå–é“¾æ¥ï¼Œçˆ¬è™«å³å¥”å‘ä¸‹ä¸€ä¸ªèœå›­ã€‚

```
    def requestNextPage(self, response):
        next_page = response.xpath('.//li[@class="next"]/a/@href').extract_first()
        # åˆ¤æ–­ä¸‹ä¸€ä¸ªæ˜¯æŒ‰é’®å…ƒç´ çš„é“¾æ¥æ˜¯å¦å­˜åœ¨
        if next_page is not None:
            if next_page != '':
                return scrapy.Request(url='http://quotes.toscrape.com/'+next_page, callback=self.parse)
        return None
```

çˆ¬è™«çš„ä¸»è¦é€»è¾‘åˆ°è¿™é‡Œå°±ç»“æŸäº†ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸€å°æ®µä»£ç å°±å¯ä»¥å®ç°ä¸€ä¸ªç®€å•çš„çˆ¬è™«ã€‚ä¸€èˆ¬ä¸»æµç½‘é¡µéƒ½é’ˆå¯¹é˜²çˆ¬è™«åšäº†ä¸€äº›å¤„ç†ï¼Œå®æ“è¿‡ç¨‹ä¸­ä¹Ÿè®¸å¹¶ä¸ä¼šè¿™ä¹ˆé¡ºåˆ©ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦æ¨¡ä»¿æµè§ˆå™¨çš„User-Agentï¼Œæˆ–è€…åšè®¿é—®å»¶æ—¶é˜²æ­¢è¯·æ±‚è¿‡äºé¢‘ç¹ç­‰ç­‰å¤„ç†ã€‚

# æ•°æ®å¤„ç†ï¼špipelines

pipelinesæ˜¯ Scrapy ç”¨æ¥åç»­å¤„ç†çš„ç®¡é“ï¼Œå¯ä»¥åŒæ—¶å­˜åœ¨å¤šä¸ªï¼Œå¹¶ä¸”å¯ä»¥è‡ªå®šä¹‰é¡ºåºæ‰§è¡Œï¼Œé€šå¸¸ç”¨æ¥åšæ•°æ®å¤„ç†å’Œæ•°æ®ä¿å­˜ã€‚æˆ‘ä»¬éœ€è¦åœ¨``settings.py``æ–‡ä»¶ä¸­è®¾ç½®éœ€è¦éœ€è¦æ‰§è¡Œçš„ç®¡é“å’Œæ‰§è¡Œé¡ºåºã€‚

```
# åœ¨ settings.py åŠ å…¥ä¸‹é¢çš„ä»£ç 
ITEM_PIPELINES = {
   'ScrapySample.pipelines.ScrapySamplePipeline': 300,
}
```
åœ¨è¿™é‡Œæˆ‘åªä½¿ç”¨äº†ä¸€ä¸ªç®¡é“``ScrapySamplePipeline``ï¼Œç”¨æ¥å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“å½“ä¸­ï¼Œåé¢çš„æ•°å­—``300``æ˜¯è¡¨ç¤ºè¯¥ç®¡é“çš„ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ã€‚

ç”±äºæˆ‘ä»¬è¦ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å…ˆåœ¨æœ¬åœ°æ­å»ºèµ·æ•°æ®åº“æœåŠ¡ï¼Œæˆ‘è¿™é‡Œç”¨çš„æ˜¯``MySQL``ï¼Œå¦‚æœæ²¡æœ‰æ­å»ºçš„å°ä¼™ä¼´å¯ä»¥ä¸‹ä¸ª [MAMP](https://www.mamp.info/en/) å…è´¹ç‰ˆæœ¬ï¼Œå®‰è£…å¥½å‚»ç“œå¼æ“ä½œä¸€é”®å¯åŠ¨``Apache``ã€``MySQL``æœåŠ¡ã€‚å½“ç„¶ï¼Œæ•°æ®åº“å’Œè¡¨è¿˜æ˜¯è¦è‡ªå·±å»ºçš„ã€‚

![MAMP](https://user-gold-cdn.xitu.io/2018/1/26/16132d56bcd4d82e?w=300&h=207&f=png&s=16004)


```
# åœ¨ pipelines.py ä¸­åŠ å…¥æ•°æ®åº“é…ç½®ä¿¡æ¯
config = {
    'host': '127.0.0.1',
    'port': 8081,
    'user': 'root',
    'password': 'root',
    'db': 'xietao',
    'charset': 'utf8mb4',
    'cursorclass': pymysql.cursors.DictCursor,
}
```

æˆ‘ä»¬å¯ä»¥åœ¨``__init__()``å‡½æ•°é‡Œåšä¸€äº›åˆå§‹åŒ–å·¥ä½œï¼Œæ¯”å¦‚è¯´è¿æ¥æ•°æ®åº“ã€‚

ç„¶å``process_item()``å‡½æ•°æ˜¯ç®¡é“å¤„ç†äº‹ä»¶çš„å‡½æ•°ï¼Œæˆ‘ä»¬è¦åœ¨è¿™é‡Œå°†æ•°æ®ä¿å­˜å…¥æ•°æ®åº“ï¼Œæˆ‘åœ¨è¿™ä¸ªå‡½æ•°é‡Œå†™äº†ä¸€äº›æ’å…¥æ•°æ®åº“æ“ä½œã€‚

``close_spider()``å‡½æ•°æ˜¯çˆ¬è™«ç»“æŸå·¥ä½œæ—¶å€™è°ƒç”¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œå…³é—­æ•°æ®åº“ã€‚

```
class ScrapySamplePipeline(object):

    def __init__(self):
        # è¿æ¥æ•°æ®åº“
        self.db = sql.connect(**config)
        self.cursor = self.db.cursor()

    def process_item(self, item, spider):
        # å…ˆä¿å­˜ä½œè€…ä¿¡æ¯
        sql = 'INSERT INTO author (name, birthday, address, detail) VALUES (%s, %s, %s, %s)'
        self.cursor.execute(sql, (item['author']['name'], item['author']['birthday'], item['author']['address'], item['author']['description']))
        # è·å–ä½œè€…id
        author_id = self.cursor.lastrowid

        # ä¿å­˜å¼•è¿°ä¿¡æ¯
        sql = 'INSERT INTO spider (text, tags, author) VALUES (%s, %s, %s)'
        self.cursor.execute(sql, (item['text'], ','.join(item['tags']), author_id))
        self.db.commit()

    # å³å°†ç»“æŸçˆ¬è™«
    def close_spider(self, spider):
        self.db.close()
        self.cursor.close()
        print('close db')
```

å¦‚æœä¸éœ€è¦ä¿å­˜æ•°æ®åº“æˆ–è€…å¯¹æ•°æ®å¤„ç†çš„è¯ï¼Œ``pipelines``è¿™éƒ¨åˆ†æ˜¯å¯ä»¥å¿½ç•¥çš„ã€‚è¿™ä¸ªæ—¶å€™åœ¨å‘½ä»¤è¡Œåˆ‡æ¢åˆ°å·¥ç¨‹ç›®å½•ä¸‹ï¼Œè¾“å…¥å¼€å§‹æ‰§è¡Œçˆ¬è™«å‘½ä»¤ï¼š
```
$ scrapy crawl quotes
```

éƒ¨åˆ†ä¸ä¿å­˜åˆ°æ•°æ®åº“çš„å°ä¼™ä¼´å¯ä»¥ä½¿ç”¨ä¸‹æ–¹å‘½ä»¤ï¼Œå°†çˆ¬å–çš„æ•°æ®ä»¥ Json æ ¼å¼å¯¼å‡ºåˆ°è¯¥å·¥ç¨‹ç›®å½•ä¸‹ã€‚

```
$ scrapy crawl quotes -o quotes.json
```

æœ€åè´´ä¸Šæ•°æ®åº“æ•°æ®æˆåŠŸå½•å…¥çš„æˆªå›¾ã€‚
![Data](https://user-gold-cdn.xitu.io/2018/1/26/161330320877c008?w=2014&h=1034&f=png&s=555547)


# æ€»ç»“

è¿™æ˜¯ä½œè€…æœ€å¼€å§‹å­¦ä¹  Python çš„æ—¶å€™å†™çš„ï¼Œæœ‰ä¸€äº›ä¸å°½äººæ„çš„åœ°æ–¹åé¢ä¼šå†è°ƒæ•´ï¼Œå†™ä¸‹æœ¬æ–‡ç”¨æ„æ˜¯å·©å›ºçŸ¥è¯†æˆ–æ˜¯ç”¨äºä»¥åå›é¡¾ï¼ŒåŒæ—¶å¸Œæœ›å¯¹åŒæ ·åˆšå¼€å§‹å­¦ä¹  Python çš„è¯»è€…æœ‰æ‰€å¸®åŠ©ã€‚

**æœ€åå†æ¬¡è´´ä¸Š[Demo](https://github.com/xietao3/ScrapySample) ï¸ã€‚**
